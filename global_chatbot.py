# âœ… ê¸°ì¤€ ë¶„ë¥˜ ê¸°ë°˜ìœ¼ë¡œ ì„ íƒëœ ê¸°ì¤€ì„œë§Œ ì²˜ë¦¬í•˜ë„ë¡ ë¦¬íŒ©í† ë§í•œ ì½”ë“œ

import os
import re
import streamlit as st
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.messages import SystemMessage, HumanMessage
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMListwiseRerank

# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv()

# âœ… ëª¨ë¸ ì •ì˜
llm = ChatOpenAI(model="gpt-4o", temperature=0)
embedding_model = OpenAIEmbeddings()

# âœ… í´ë¦°ì—… í•¨ìˆ˜
def clean_text(text: str) -> str:
    lines = text.splitlines()
    cleaned = []
    for line in lines:
        stripped = line.strip()
        if not stripped:
            continue
        if re.match(r"^\d{1,2}$", stripped):
            continue
        cleaned.append(stripped)
    return ' '.join(cleaned)

# âœ… ê¸°ì¤€ì„œë³„ ë²¡í„° DB ë¡œë“œ
vectorstore_paths = {
    "GRI": r"D:\bit_esg\python\esg_chatbot_project\src\vector_finish\vectorstores\GRI_01_2nd",
    "IFRS_S1": r"D:\bit_esg\python\esg_chatbot_project\src\vector_finish\vectorstores\IFRS_01_2nd",
    "IFRS_S2": r"D:\bit_esg\python\esg_chatbot_project\src\vector_finish\vectorstores\IFRS_02_2nd",
    "TCFD": r"D:\bit_esg\python\esg_chatbot_project\src\vector_finish\vectorstores\TFCD_01_2nd",
    "KSSB1": r"D:\bit_esg\python\esg_chatbot_project\src\vector_finish\vectorstores\KSSB_01_2nd",
    "KSSB2": r"D:\bit_esg\python\esg_chatbot_project\src\vector_finish\vectorstores\KSSB_02_2nd",
    "KSSB101": r"D:\bit_esg\python\esg_chatbot_project\src\vector_finish\vectorstores\KSSB_101_2nd"
}

dbs = {
    name: FAISS.load_local(path, embedding_model, allow_dangerous_deserialization=True)
    for name, path in vectorstore_paths.items()
}

# âœ… GPT ë²ˆì—­ í•¨ìˆ˜
def translate_to_english(korean_query: str) -> str:
    messages = [
        SystemMessage(content="You are a professional English translator. " \
        "Translate the following Korean question into natural and accurate English."),
        HumanMessage(content=korean_query)
    ]
    return llm(messages).content.strip()

# âœ… ê¸°ì¤€ í‚¤ì›Œë“œ ë¶„ë¥˜ í•¨ìˆ˜
# def extract_relevant_sentences(text, keywords, max_sentences=3):
#     """
#     ì£¼ì–´ì§„ ì‘ë‹µì—ì„œ keywordsì™€ ì—°ê´€ëœ í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œ
#     """
#     sentences = re.split(r'(?<=[.!?])\s+', text)
#     scored = []

#     for sent in sentences:
#         score = sum(kw.lower() in sent.lower() for kw in keywords)
#         if score > 0:
#             scored.append((score, sent))

#     # ì ìˆ˜ ìˆœ ì •ë ¬ í›„ ìƒìœ„ max_sentencesë§Œ ë°˜í™˜
#     top_sentences = [s for _, s in sorted(scored, key=lambda x: -x[0])[:max_sentences]]
#     return " ".join(top_sentences)


# âœ… ê¸°ì¤€ ë¶„ë¥˜ í•¨ìˆ˜

def classify_relevant_standards(query: str):
    prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ” ESG ê¸°ì¤€ì„œë¥¼ ì•„ë˜ ì¤‘ì—ì„œ ì„ íƒí•´ì¤˜. ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥í•´.
ê°€ëŠ¥í•œ ì„ íƒ: GRI, IFRS_S1, IFRS_S2, TCFD, KSSB1, KSSB2, KSSB101
í˜•ì‹: [\"GRI\", \"IFRS_S2\"] ì²˜ëŸ¼ Python ë¦¬ìŠ¤íŠ¸ë¡œë§Œ ë‹µí•´ì¤˜.
"""
    response = llm([
        SystemMessage(content="ESG ê¸°ì¤€ì„œ ë¼ìš°íŒ… ì „ë¬¸ê°€"),
        HumanMessage(content=prompt)
    ])
    try:
        return eval(response.content)
    except:
        return list(dbs.keys())


def translate_to_korean(english_text: str) -> str:
    messages = [
        SystemMessage(content="You are a professional Korean translator. Translate the following English ESG-related paragraph into accurate Korean."),
        HumanMessage(content=english_text)
    ]
    return llm(messages).content.strip()

def is_comparative_question_via_llm(query: str) -> bool:
    messages = [
        SystemMessage(content="ë„ˆëŠ” ESG ê³µì‹œ ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì•¼. ì‚¬ìš©ì ì§ˆë¬¸ì´ 'ê¸°ì¤€ì„œ ê°„ ë¹„êµ'ë¥¼ ìš”êµ¬í•˜ëŠ”ì§€ íŒë‹¨í•´ì¤˜."),
        HumanMessage(content=f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ì´ ì§ˆë¬¸ì´ ê¸°ì¤€ì„œ(GRI, IFRS, KSSB ë“±) ê°„ ë¹„êµë¥¼ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸ì´ë©´ 'True'ë¼ê³ ë§Œ ëŒ€ë‹µí•´ì¤˜.  
ë¹„êµê°€ ì•„ë‹Œ ë‹¨ì¼ ê¸°ì¤€ì„œ ì„¤ëª…ì´ë©´ 'False'ë¼ê³ ë§Œ ëŒ€ë‹µí•´ì¤˜.
""")
    ]
    try:
        response = llm(messages).content.strip()
        return response.lower() == "true"
    except:
        return False




# âœ… ê¸°ì¤€ë³„ ì‘ë‹µ ì²˜ë¦¬ í•¨ìˆ˜
def process_standard(standard, db, query_ko, query_en):
    try:
        multi_query_retriever = MultiQueryRetriever.from_llm(
            retriever=db.as_retriever(search_kwargs={"k": 4}),
            llm=llm
        )
        reranker = LLMListwiseRerank.from_llm(llm=llm, top_n=2)
        compression_retriever = ContextualCompressionRetriever(
            base_retriever=multi_query_retriever,
            base_compressor=reranker
        )
        docs = compression_retriever.invoke(query_en)

        if not docs:
            return (standard, None, [])

        context_blocks = []
        references = []
        for doc in docs:
            meta = doc.metadata
            source = meta.get("source", "")
            citation = f"[ì¶œì²˜: {standard} - {source or 'í•´ë‹¹ ë¬¸ë‹¨'}]"
            context_blocks.append(f"{citation}\n{doc.page_content}")
            references.append({
                "standard": standard,
                "source": source or "ì•Œ ìˆ˜ ì—†ìŒ",
                "content": doc.page_content
            })

        context = "\n\n".join(context_blocks)
        system_msg = SystemMessage(
            # content=f"You are a professional ESG expert specialized in {standard} reporting standards. Based on the documents below, answer the Korean user query.\n\n[Documents]\n{context}\n\n[Original Korean Query]\n{query_ko}"
        content=f"""You are a professional ESG expert specialized in {standard} reporting standards.
                    You must ONLY use the following documents to answer the Korean query.
                    Do NOT add any external knowledge or assumptions beyond what is explicitly stated in the documents.



                    [Documents]
                    {context}

                    [Original Korean Query]
                    {query_ko}
                    """)
        user_msg = HumanMessage(content=query_ko)
        response = llm([system_msg, user_msg])
        return (standard, response.content, references)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ - {standard}: {e}")
        return (standard, None, [])

# âœ… ê¸°ì¤€ë³„ ì‘ë‹µ ìƒì„± (ì„ íƒ ê¸°ì¤€ë§Œ)
# âœ… ê¸°ì¤€ë³„ ì‘ë‹µ ìƒì„± (ì„ íƒ ê¸°ì¤€ë§Œ)
def generate_response_for_each_standard(query_ko: str, selected_standards: list):
    query_en = translate_to_english(query_ko)
    responses = {}
    documents = {}

    with ThreadPoolExecutor() as executor:
        futures = [
            executor.submit(process_standard, std, dbs[std], query_ko, query_en)
            for std in selected_standards if std in dbs
        ]
        for future in futures:
            std, resp, refs = future.result()
            if resp:
                responses[std] = resp
                documents[std] = refs
            else:
                print(f"â›” ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ: {std}, ì‘ë‹µ ì œì™¸ë¨")

    # âœ… ê¸°ì¤€ë³„ ì‘ë‹µ ë¹„êµ ìš”ì•½ ìƒì„± (ë‹¨ì¼ ë¬¸ì¥ í˜•íƒœ)
    try:
        summary_prompt = f"""
ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ê° ESG ê¸°ì¤€ì„œ(GRI, IFRS ë“±)ë¡œë¶€í„° ìˆ˜ì§‘í•œ ì‘ë‹µì…ë‹ˆë‹¤.
ì´ ì‘ë‹µë“¤ì„ ë¹„êµí•˜ì—¬ ê³µí†µì  ë˜ëŠ” ì°¨ì´ì ì„ ê°„ê²°í•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.

[ì§ˆë¬¸]
{query_ko}

[ê¸°ì¤€ë³„ ì‘ë‹µë“¤]
"""
        for std, resp in responses.items():
            summary_prompt += f"\n### {std} ì‘ë‹µ:\n{resp}\n"

        messages = [
            SystemMessage(content="ë„ˆëŠ” ESG ê¸°ì¤€ì„œ ë¹„êµ ì „ë¬¸ê°€ì•¼. ì•„ë˜ ê¸°ì¤€ë³„ ì‘ë‹µì„ ë¹„êµí•œ ìš”ì•½ì„ ë‹¨ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§í•´ì¤˜."),
            HumanMessage(content=summary_prompt)
        ]
        summary_sentence = llm(messages).content.strip()
        responses["summary"] = summary_sentence  # âœ… ì—¬ê¸°ì— ìµœì¢… ë¬¸ì¥ ì‚½ì…
    except Exception as e:
        print(f"âš ï¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
        responses["summary"] = "âš ï¸ ê¸°ì¤€ì„œ ê°„ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

    return responses, documents

import difflib

def is_similar_sentence(a: str, b: str, threshold: float = 0.8) -> bool:
    return difflib.SequenceMatcher(None, a, b).ratio() > threshold


# # âœ… ê¸°ì¤€ë³„ ìš”ì•½ ì‘ë‹µ ìƒì„±
# def generate_comparative_summary(responses_dict: dict, original_query_ko: str):
#     filtered_responses = {
#         std: resp for std, resp in responses_dict.items()
#         if "ê´€ë ¨ ë‚´ìš© ì—†ìŒ" not in resp and resp.strip()
#     }

#     # âœ… ì§ˆì˜ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ìˆœí•˜ê²Œ ë„ì–´ì“°ê¸° ê¸°ì¤€ ë¶„ë¦¬ ë˜ëŠ” tokenizer ì ìš©)
#     keywords = original_query_ko.strip().split()  # ë˜ëŠ” Okt/nltk ë“± ì‚¬ìš© ê°€ëŠ¥

#     prompt = f"""
# [ì‚¬ìš©ì ì§ˆë¬¸]
# {original_query_ko}

# [ì‘ë‹µ ì§€ì‹œì‚¬í•­]
# - ì•„ë˜ ê° ê¸°ì¤€({', '.join(filtered_responses.keys())})ì— ëŒ€í•œ ì‘ë‹µ ì¤‘ì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ **ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ì¶”ë ¤ì„œ** ë¹„êµí•´ì¤˜.
# - ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ì¼ë°˜ ì„¤ëª…ì€ ì œê±°í•˜ê³ , **ì§ˆë¬¸ í‚¤ì›Œë“œì™€ ì˜ë¯¸ìƒ ì—°ê²°ëœ ë‚´ìš©**ë§Œ ë‚¨ê²¨ì¤˜.
# - ê¸°ì¤€ë³„ë¡œ 1~2ë¬¸ì¥ì”© ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì¤˜.
# - ë¬¸ì„œì— **ì§ì ‘ ì–¸ê¸‰ëœ ë‚´ìš©ë§Œ** ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•  ê²ƒ
# - ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” â€œë¬¸ì„œì— ì–¸ê¸‰ë˜ì§€ ì•ŠìŒâ€ì´ë¼ê³  ë§í•  ê²ƒ
# - ê´€ë ¨ ë‚´ìš©ì´ ì—†ëŠ” ê¸°ì¤€ì€ "ê´€ë ¨ ë‚´ìš© ì—†ìŒ"ì´ë¼ê³  í‘œì‹œí•´ì¤˜.

# [ê¸°ì¤€ë³„ ì‘ë‹µë“¤]
# """
#     for std, resp in filtered_responses.items():
#         refined_resp = extract_relevant_sentences(resp, keywords)
#         prompt += f"\n### {std} ì‘ë‹µ:\n{refined_resp}\n"

#     messages = [
#         SystemMessage(content="ë„ˆëŠ” ESG ê³µì‹œ ê¸°ì¤€ ì „ë¬¸ê°€ì•¼. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë”°ë¼ ê¸°ì¤€ë³„ ì‘ë‹µì„ ìš”ì•½Â·ë¹„êµí•´ì¤˜."),
#         HumanMessage(content=prompt)
#     ]
#     return llm(messages).content


# âœ… Streamlit ì•± ì‹¤í–‰ í•¨ìˆ˜
# def run_global_chatbot():
#     st.header("ESG Chatbot")
#     query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œì„ ì–´ë–»ê²Œ ê³µì‹œí•´ì•¼ í•˜ë‚˜ìš”?)")

#     if query:
#         with st.spinner("ì§ˆë¬¸ ë¶„ì„ ì¤‘: ê´€ë ¨ ê¸°ì¤€ì„œë¥¼ ì‹ë³„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
#             selected = classify_relevant_standards(query)
#             st.markdown(f"âœ… ì„ íƒëœ ê¸°ì¤€ì„œ: {', '.join(selected)}")

#         with st.spinner("ê¸°ì¤€ë³„ ë¬¸ì„œ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± ì¤‘..."):
#             responses, references = generate_response_for_each_standard(query, selected)

#         with st.expander("ğŸ“„ ê¸°ì¤€ë³„ ìƒì„¸ ì‘ë‹µ ë³´ê¸°"):
#             for std, resp in responses.items():
#                 st.markdown(f"### {std} ì‘ë‹µ")
#                 st.write(resp)

#         with st.expander("ğŸ“š ì¸ìš©ëœ ë¬¸ì„œ ì¶œì²˜ ë³´ê¸°"):
#             for std, refs in references.items():
#                 st.markdown(f"## {std} ë¬¸ì„œ ì¸ìš©")
#                 for ref in refs:
#                     cleaned_text = clean_text(ref['content'])
#                     content_html = f"""
#                     <div style='margin-bottom:1.5em; padding:1em; border:1px solid #ccc; border-radius:10px; background-color:#f9f9f9'>
#                         <p><b>ğŸ“Œ ì†ŒìŠ¤:</b> {ref.get("source", "ì—†ìŒ")}</p>
#                         <pre style='white-space: pre-wrap; font-size: 14px; line-height: 1.5;'>{cleaned_text}</pre>
#                     </div>
#                     """
#                     st.markdown(content_html, unsafe_allow_html=True)


def run_eng_chatbot_app():
    import os
    import re
    import streamlit as st
    from io import BytesIO
    import pythoncom
    from docx import Document
    import tempfile
    from docx2pdf import convert

    def remove_control_characters(text: str) -> str:
        return re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f]', '', text)

    # st.set_page_config(page_title="ESG ê³µì‹œ ë¹„êµ ì±—ë´‡", layout="wide")
    st.title("ğŸŒG-Mate")

    user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œì„ ì–´ë–»ê²Œ ê³µì‹œí•´ì•¼ í•˜ë‚˜ìš”?)")

    # if st.button("ì…ë ¥") and user_input:
    #     with st.spinner("ì§ˆë¬¸ ë¶„ì„ ì¤‘: ê´€ë ¨ ê¸°ì¤€ì„œë¥¼ ì‹ë³„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
    #         selected = classify_relevant_standards(user_input)
    #         st.markdown(f"âœ… ì„ íƒëœ ê¸°ì¤€ì„œ: {', '.join(selected) if selected else 'ì—†ìŒ'}")

    #     with st.spinner("ê¸°ì¤€ë³„ ë¬¸ì„œ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± ì¤‘..."):
    #         responses, references = generate_response_for_each_standard(user_input, selected)
        
    #     st.session_state.responses = responses
    #     st.session_state.references = references
    #     st.session_state.user_input = user_input

    if st.button("ì…ë ¥") and user_input:
        with st.spinner("ì§ˆë¬¸ ë¶„ì„ ì¤‘: ê´€ë ¨ ê¸°ì¤€ì„œë¥¼ ì‹ë³„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            # âœ… ë¹„êµ ì§ˆë¬¸ ì—¬ë¶€ íŒë‹¨
            is_comparative = is_comparative_question_via_llm(user_input)
            st.session_state.is_comparative = is_comparative
            if is_comparative:
                st.info("ğŸ“Œ ì´ ì§ˆë¬¸ì€ ê¸°ì¤€ì„œ ê°„ ë¹„êµ ì§ˆë¬¸ì…ë‹ˆë‹¤.")

            selected = classify_relevant_standards(user_input)
            st.markdown(f"âœ… ì„ íƒëœ ê¸°ì¤€ì„œ: {', '.join(selected) if selected else 'ì—†ìŒ'}")

        with st.spinner("ê¸°ì¤€ë³„ ë¬¸ì„œ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± ì¤‘..."):
            responses, references = generate_response_for_each_standard(user_input, selected)

        st.session_state.responses = responses
        st.session_state.references = references
        st.session_state.user_input = user_input



    if "user_input" in st.session_state:
        responses = st.session_state.responses
        references = st.session_state.references
        user_input = st.session_state.user_input

        # st.header(f"ğŸ“„ ê¸°ì¤€ë³„ ìƒì„¸ ì‘ë‹µ ë³´ê¸°")
        # for std, resp in responses.items():
        #     st.markdown(f"### {std} ì‘ë‹µ")
        #     st.write(resp)
        for std, resp in responses.items():
            for ref in references.get(std, []):
                raw_english = ref['content'].strip()

                # âœ… ì˜ì–´ ë¬¸ë‹¨ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­
                translated = translate_to_korean(raw_english).strip()
                # âœ… ìœ ì‚¬ë„ ê¸°ë°˜ ë¹„êµ
                if not is_similar_sentence(translated, resp):
                    print(f"[âš ï¸] {std} ì‘ë‹µì— ì¸ìš©ëœ ë¬¸ë‹¨ì´ í¬í•¨ë˜ì§€ ì•ŠìŒ")


        if responses:
            st.header("ğŸ“„ ê¸°ì¤€ë³„ ìƒì„¸ ì‘ë‹µ ë³´ê¸°")
            for std, resp in responses.items():
                st.markdown(f"### {std} ì‘ë‹µ")
                st.write(resp)
        else:
            st.header("ğŸ“„ ê¸°ì¤€ë³„ ìƒì„¸ ì‘ë‹µ ì—†ìŒ")

        with st.expander("ğŸ“š ì¸ìš©ëœ ë¬¸ì„œ ì¶œì²˜ ë³´ê¸°"):
            for std, refs in references.items():
                st.markdown(f"## {std} ë¬¸ì„œ ì¸ìš©")
                for ref in refs:
                    cleaned_text = clean_text(ref['content'])
                    content_html = f"""
                    <div style='margin-bottom:1.5em; padding:1em; border:1px solid #ccc; border-radius:10px; background-color:#f9f9f9'>
                        <p><b>ğŸ“Œ ì†ŒìŠ¤:</b> {ref.get("source", "ì—†ìŒ")}</p>
                        <pre style='white-space: pre-wrap; font-size: 14px; color: auto; line-height: 1.5;'>{cleaned_text}</pre>
                    </div>
                    """
                    st.markdown(content_html, unsafe_allow_html=True)

        # Word ë¬¸ì„œ ìƒì„±
        doc = Document()
        doc.add_heading('ESG ê³µì‹œ ë¹„êµ ì±—ë´‡ ê²°ê³¼', 0)
        doc.add_heading('ì§ˆë¬¸', level=1)
        doc.add_paragraph(user_input)
        doc.add_heading('ğŸ“„ ê¸°ì¤€ë³„ ì‘ë‹µ', level=1)
        # if "summary" in responses: ##ì¶”ê°€
        #     st.subheader("ğŸ§© ê¸°ì¤€ì„œ ê°„ ì‘ë‹µ ìš”ì•½")
        #     st.write(responses["summary"])

        for std, resp in responses.items():
            doc.add_heading(std, level=2)
            doc.add_paragraph(resp)
        doc.add_heading('ğŸ“š ì¸ìš© ë¬¸ì„œ', level=1)
        for std, refs in references.items():
            doc.add_heading(std, level=2)
            for ref in refs:
                doc.add_paragraph(f"ì¶œì²˜: {ref['source']}")
                safe_text = remove_control_characters(clean_text(ref['content']))
                doc.add_paragraph(safe_text)

        # Word â†’ BytesIO
        doc_io = BytesIO()
        doc.save(doc_io)
        doc_io.seek(0)

        st.download_button(
            label="â¬‡ï¸ Word íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
            data=doc_io,
            file_name="esg_ë‹µë³€ê¸°ë¡.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )

        # PDF ë‹¤ìš´ë¡œë“œ
        def convert_docx_to_pdf(doc: Document) -> BytesIO:
            pythoncom.CoInitialize()
            with tempfile.TemporaryDirectory() as tmpdir:
                docx_path = os.path.join(tmpdir, "temp.docx")
                pdf_path = os.path.join(tmpdir, "temp.pdf")
                doc.save(docx_path)
                convert(docx_path, pdf_path)
                with open(pdf_path, "rb") as f:
                    return BytesIO(f.read())

        # âœ… PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì²˜ë¦¬
        try:
            pdf_bytes = convert_docx_to_pdf(doc)
            st.download_button(
                label="â¬‡ï¸ PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                data=pdf_bytes,
                file_name="esg_ë‹µë³€ê¸°ë¡.pdf",
                mime="application/pdf"
            )
        except Exception as e:
            st.warning("âš ï¸ PDF ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. Word íŒŒì¼ì€ ì •ìƒ ì €ì¥ë©ë‹ˆë‹¤.")
            st.text(str(e))
